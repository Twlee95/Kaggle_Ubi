{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import joblib\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from argparse import Namespace\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "from scipy import stats\n",
    " \n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit, StratifiedKFold, train_test_split\n",
    "\n",
    "from pytorch_tabnet.metrics import Metric\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "from pytorch_tabnet.pretraining import TabNetPretrainer\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('max_columns', 64)\n",
    "\n",
    "def seed_everything(seed: int = 42) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    INFER=True,\n",
    "    debug=False,\n",
    "    seed=21,\n",
    "    folds=5,\n",
    "    workers=4,\n",
    "    min_time_id=None, \n",
    "    holdout=True,\n",
    "    num_bins=16,\n",
    "    data_path=Path(\"../input/ubiquant-market-prediction-half-precision-pickle\"),\n",
    "    dnn_path = '../input/dnnmodel',\n",
    "    tabnet_path = '../input/ubiquanttabnetbaseline'\n",
    ")\n",
    "seed_everything(args.seed)\n",
    "\n",
    "if args.debug:\n",
    "    setattr(args, 'min_time_id', 1100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_id</th>\n",
       "      <th>investment_id</th>\n",
       "      <th>target</th>\n",
       "      <th>f_0</th>\n",
       "      <th>f_1</th>\n",
       "      <th>f_2</th>\n",
       "      <th>f_3</th>\n",
       "      <th>f_4</th>\n",
       "      <th>f_5</th>\n",
       "      <th>f_6</th>\n",
       "      <th>f_7</th>\n",
       "      <th>f_8</th>\n",
       "      <th>f_9</th>\n",
       "      <th>f_10</th>\n",
       "      <th>f_11</th>\n",
       "      <th>f_12</th>\n",
       "      <th>f_13</th>\n",
       "      <th>f_14</th>\n",
       "      <th>f_15</th>\n",
       "      <th>f_16</th>\n",
       "      <th>f_17</th>\n",
       "      <th>f_18</th>\n",
       "      <th>f_19</th>\n",
       "      <th>f_20</th>\n",
       "      <th>f_21</th>\n",
       "      <th>f_22</th>\n",
       "      <th>f_23</th>\n",
       "      <th>f_24</th>\n",
       "      <th>f_25</th>\n",
       "      <th>f_26</th>\n",
       "      <th>f_27</th>\n",
       "      <th>f_28</th>\n",
       "      <th>...</th>\n",
       "      <th>f_268</th>\n",
       "      <th>f_269</th>\n",
       "      <th>f_270</th>\n",
       "      <th>f_271</th>\n",
       "      <th>f_272</th>\n",
       "      <th>f_273</th>\n",
       "      <th>f_274</th>\n",
       "      <th>f_275</th>\n",
       "      <th>f_276</th>\n",
       "      <th>f_277</th>\n",
       "      <th>f_278</th>\n",
       "      <th>f_279</th>\n",
       "      <th>f_280</th>\n",
       "      <th>f_281</th>\n",
       "      <th>f_282</th>\n",
       "      <th>f_283</th>\n",
       "      <th>f_284</th>\n",
       "      <th>f_285</th>\n",
       "      <th>f_286</th>\n",
       "      <th>f_287</th>\n",
       "      <th>f_288</th>\n",
       "      <th>f_289</th>\n",
       "      <th>f_290</th>\n",
       "      <th>f_291</th>\n",
       "      <th>f_292</th>\n",
       "      <th>f_293</th>\n",
       "      <th>f_294</th>\n",
       "      <th>f_295</th>\n",
       "      <th>f_296</th>\n",
       "      <th>f_297</th>\n",
       "      <th>f_298</th>\n",
       "      <th>f_299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.300781</td>\n",
       "      <td>0.932573</td>\n",
       "      <td>0.113691</td>\n",
       "      <td>-0.402206</td>\n",
       "      <td>0.378386</td>\n",
       "      <td>-0.203938</td>\n",
       "      <td>-0.413469</td>\n",
       "      <td>0.965623</td>\n",
       "      <td>1.230508</td>\n",
       "      <td>0.114809</td>\n",
       "      <td>-2.012777</td>\n",
       "      <td>0.004936</td>\n",
       "      <td>0.284220</td>\n",
       "      <td>0.502155</td>\n",
       "      <td>-0.287932</td>\n",
       "      <td>-1.169338</td>\n",
       "      <td>-0.267310</td>\n",
       "      <td>-0.574423</td>\n",
       "      <td>-0.771869</td>\n",
       "      <td>1.012212</td>\n",
       "      <td>-1.230507</td>\n",
       "      <td>1.785726</td>\n",
       "      <td>-2.090686</td>\n",
       "      <td>0.325659</td>\n",
       "      <td>-0.877769</td>\n",
       "      <td>1.048786</td>\n",
       "      <td>0.131774</td>\n",
       "      <td>-0.349609</td>\n",
       "      <td>-1.813385</td>\n",
       "      <td>0.099226</td>\n",
       "      <td>...</td>\n",
       "      <td>0.900593</td>\n",
       "      <td>-0.924766</td>\n",
       "      <td>-1.057890</td>\n",
       "      <td>-0.167062</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.281245</td>\n",
       "      <td>0.258715</td>\n",
       "      <td>-0.237964</td>\n",
       "      <td>-0.742125</td>\n",
       "      <td>-0.324677</td>\n",
       "      <td>0.992547</td>\n",
       "      <td>0.961355</td>\n",
       "      <td>-0.025610</td>\n",
       "      <td>-0.006259</td>\n",
       "      <td>0.473603</td>\n",
       "      <td>0.040136</td>\n",
       "      <td>0.453711</td>\n",
       "      <td>-1.597790</td>\n",
       "      <td>0.301659</td>\n",
       "      <td>0.157470</td>\n",
       "      <td>0.416631</td>\n",
       "      <td>1.506131</td>\n",
       "      <td>0.366028</td>\n",
       "      <td>-1.095620</td>\n",
       "      <td>0.200075</td>\n",
       "      <td>0.819155</td>\n",
       "      <td>0.941183</td>\n",
       "      <td>-0.086764</td>\n",
       "      <td>-1.087009</td>\n",
       "      <td>-1.044826</td>\n",
       "      <td>-0.287605</td>\n",
       "      <td>0.321566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.231079</td>\n",
       "      <td>0.810802</td>\n",
       "      <td>-0.514115</td>\n",
       "      <td>0.742368</td>\n",
       "      <td>-0.616673</td>\n",
       "      <td>-0.194255</td>\n",
       "      <td>1.771210</td>\n",
       "      <td>1.428127</td>\n",
       "      <td>1.134144</td>\n",
       "      <td>0.114809</td>\n",
       "      <td>-0.219201</td>\n",
       "      <td>-0.351726</td>\n",
       "      <td>0.846882</td>\n",
       "      <td>0.440299</td>\n",
       "      <td>0.499824</td>\n",
       "      <td>0.893144</td>\n",
       "      <td>-0.010217</td>\n",
       "      <td>-0.681523</td>\n",
       "      <td>1.254092</td>\n",
       "      <td>-1.026969</td>\n",
       "      <td>-1.690156</td>\n",
       "      <td>0.011152</td>\n",
       "      <td>0.875251</td>\n",
       "      <td>0.325659</td>\n",
       "      <td>-0.458305</td>\n",
       "      <td>-1.797581</td>\n",
       "      <td>-0.300364</td>\n",
       "      <td>0.584786</td>\n",
       "      <td>0.551460</td>\n",
       "      <td>0.806422</td>\n",
       "      <td>...</td>\n",
       "      <td>0.171233</td>\n",
       "      <td>1.165891</td>\n",
       "      <td>0.590802</td>\n",
       "      <td>0.118520</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.650803</td>\n",
       "      <td>0.851905</td>\n",
       "      <td>0.086198</td>\n",
       "      <td>1.135668</td>\n",
       "      <td>0.298990</td>\n",
       "      <td>-1.583445</td>\n",
       "      <td>-0.481945</td>\n",
       "      <td>0.532229</td>\n",
       "      <td>0.226693</td>\n",
       "      <td>-0.894744</td>\n",
       "      <td>-0.514552</td>\n",
       "      <td>-1.000073</td>\n",
       "      <td>0.884377</td>\n",
       "      <td>-0.557502</td>\n",
       "      <td>-0.875265</td>\n",
       "      <td>-0.156106</td>\n",
       "      <td>0.537055</td>\n",
       "      <td>-0.154193</td>\n",
       "      <td>0.912726</td>\n",
       "      <td>-0.734579</td>\n",
       "      <td>0.819155</td>\n",
       "      <td>0.941183</td>\n",
       "      <td>-0.387617</td>\n",
       "      <td>-1.087009</td>\n",
       "      <td>-0.929529</td>\n",
       "      <td>-0.974060</td>\n",
       "      <td>-0.343624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.568848</td>\n",
       "      <td>0.393974</td>\n",
       "      <td>0.615937</td>\n",
       "      <td>0.567806</td>\n",
       "      <td>-0.607963</td>\n",
       "      <td>0.068883</td>\n",
       "      <td>-1.083155</td>\n",
       "      <td>0.979656</td>\n",
       "      <td>-1.125681</td>\n",
       "      <td>0.114809</td>\n",
       "      <td>-1.035376</td>\n",
       "      <td>0.465096</td>\n",
       "      <td>0.150847</td>\n",
       "      <td>-0.044009</td>\n",
       "      <td>0.091257</td>\n",
       "      <td>-1.169338</td>\n",
       "      <td>-0.451820</td>\n",
       "      <td>-0.467322</td>\n",
       "      <td>0.095288</td>\n",
       "      <td>1.140719</td>\n",
       "      <td>-0.166894</td>\n",
       "      <td>-0.007295</td>\n",
       "      <td>-0.449418</td>\n",
       "      <td>0.325659</td>\n",
       "      <td>-0.682472</td>\n",
       "      <td>0.016262</td>\n",
       "      <td>0.026124</td>\n",
       "      <td>-0.547330</td>\n",
       "      <td>0.551460</td>\n",
       "      <td>-0.261588</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.161757</td>\n",
       "      <td>-0.114321</td>\n",
       "      <td>0.433277</td>\n",
       "      <td>-0.207127</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.098910</td>\n",
       "      <td>-0.725177</td>\n",
       "      <td>-0.388189</td>\n",
       "      <td>0.062644</td>\n",
       "      <td>0.260281</td>\n",
       "      <td>0.980960</td>\n",
       "      <td>0.899393</td>\n",
       "      <td>-0.315315</td>\n",
       "      <td>0.150139</td>\n",
       "      <td>0.245546</td>\n",
       "      <td>-1.429645</td>\n",
       "      <td>-1.000073</td>\n",
       "      <td>-0.033494</td>\n",
       "      <td>-0.147156</td>\n",
       "      <td>-0.087518</td>\n",
       "      <td>0.098443</td>\n",
       "      <td>-0.529027</td>\n",
       "      <td>-0.138020</td>\n",
       "      <td>0.912726</td>\n",
       "      <td>-0.551904</td>\n",
       "      <td>-1.220772</td>\n",
       "      <td>-1.060166</td>\n",
       "      <td>-0.219097</td>\n",
       "      <td>-1.087009</td>\n",
       "      <td>-0.612428</td>\n",
       "      <td>-0.113944</td>\n",
       "      <td>0.243608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-1.064453</td>\n",
       "      <td>-2.343535</td>\n",
       "      <td>-0.011870</td>\n",
       "      <td>1.874606</td>\n",
       "      <td>-0.606346</td>\n",
       "      <td>-0.586827</td>\n",
       "      <td>-0.815737</td>\n",
       "      <td>0.778096</td>\n",
       "      <td>0.298990</td>\n",
       "      <td>0.114809</td>\n",
       "      <td>-1.176410</td>\n",
       "      <td>0.301204</td>\n",
       "      <td>-0.015678</td>\n",
       "      <td>-0.656295</td>\n",
       "      <td>-0.150829</td>\n",
       "      <td>-1.169338</td>\n",
       "      <td>-1.121598</td>\n",
       "      <td>-0.681523</td>\n",
       "      <td>2.921973</td>\n",
       "      <td>1.179453</td>\n",
       "      <td>0.363938</td>\n",
       "      <td>0.365656</td>\n",
       "      <td>-1.478154</td>\n",
       "      <td>0.325659</td>\n",
       "      <td>0.955586</td>\n",
       "      <td>-0.105029</td>\n",
       "      <td>-0.588826</td>\n",
       "      <td>0.650101</td>\n",
       "      <td>-1.813385</td>\n",
       "      <td>-1.421967</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.024947</td>\n",
       "      <td>-0.924766</td>\n",
       "      <td>1.013206</td>\n",
       "      <td>-0.324558</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.817650</td>\n",
       "      <td>-0.624180</td>\n",
       "      <td>0.012915</td>\n",
       "      <td>0.599156</td>\n",
       "      <td>-1.796107</td>\n",
       "      <td>-1.843759</td>\n",
       "      <td>2.927223</td>\n",
       "      <td>-0.640443</td>\n",
       "      <td>2.324936</td>\n",
       "      <td>-0.666686</td>\n",
       "      <td>-0.049966</td>\n",
       "      <td>-1.000073</td>\n",
       "      <td>-0.172537</td>\n",
       "      <td>-0.557502</td>\n",
       "      <td>-0.986692</td>\n",
       "      <td>-0.744752</td>\n",
       "      <td>0.237733</td>\n",
       "      <td>0.382201</td>\n",
       "      <td>0.912726</td>\n",
       "      <td>-0.266359</td>\n",
       "      <td>-1.220772</td>\n",
       "      <td>0.941183</td>\n",
       "      <td>-0.609113</td>\n",
       "      <td>0.104928</td>\n",
       "      <td>-0.783423</td>\n",
       "      <td>1.151730</td>\n",
       "      <td>-0.773309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-0.531738</td>\n",
       "      <td>0.842057</td>\n",
       "      <td>-0.262993</td>\n",
       "      <td>2.330030</td>\n",
       "      <td>-0.583422</td>\n",
       "      <td>-0.618392</td>\n",
       "      <td>-0.742814</td>\n",
       "      <td>-0.946789</td>\n",
       "      <td>1.230508</td>\n",
       "      <td>0.114809</td>\n",
       "      <td>-0.005858</td>\n",
       "      <td>0.498780</td>\n",
       "      <td>-0.235516</td>\n",
       "      <td>-0.173336</td>\n",
       "      <td>0.106523</td>\n",
       "      <td>-0.138097</td>\n",
       "      <td>-0.829021</td>\n",
       "      <td>-0.826026</td>\n",
       "      <td>4.070158</td>\n",
       "      <td>0.148330</td>\n",
       "      <td>0.269455</td>\n",
       "      <td>-0.046685</td>\n",
       "      <td>0.194325</td>\n",
       "      <td>0.325659</td>\n",
       "      <td>1.933630</td>\n",
       "      <td>-1.119676</td>\n",
       "      <td>-0.585332</td>\n",
       "      <td>0.535749</td>\n",
       "      <td>0.551460</td>\n",
       "      <td>-0.771058</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.508002</td>\n",
       "      <td>1.165891</td>\n",
       "      <td>0.941384</td>\n",
       "      <td>-0.501547</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.332959</td>\n",
       "      <td>-0.767390</td>\n",
       "      <td>-0.237964</td>\n",
       "      <td>0.330900</td>\n",
       "      <td>-1.465071</td>\n",
       "      <td>-0.585437</td>\n",
       "      <td>-1.314835</td>\n",
       "      <td>-0.781270</td>\n",
       "      <td>-1.918721</td>\n",
       "      <td>-0.438628</td>\n",
       "      <td>-1.576060</td>\n",
       "      <td>-1.226833</td>\n",
       "      <td>1.067312</td>\n",
       "      <td>-0.519032</td>\n",
       "      <td>-1.221407</td>\n",
       "      <td>-0.768617</td>\n",
       "      <td>-0.723919</td>\n",
       "      <td>-0.170365</td>\n",
       "      <td>0.912726</td>\n",
       "      <td>-0.741355</td>\n",
       "      <td>-1.220772</td>\n",
       "      <td>0.941183</td>\n",
       "      <td>-0.588445</td>\n",
       "      <td>0.104928</td>\n",
       "      <td>0.753279</td>\n",
       "      <td>1.345611</td>\n",
       "      <td>-0.737624</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 303 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   time_id  investment_id    target       f_0       f_1       f_2       f_3  \\\n",
       "0      0.0            1.0 -0.300781  0.932573  0.113691 -0.402206  0.378386   \n",
       "1      0.0            2.0 -0.231079  0.810802 -0.514115  0.742368 -0.616673   \n",
       "2      0.0            6.0  0.568848  0.393974  0.615937  0.567806 -0.607963   \n",
       "3      0.0            7.0 -1.064453 -2.343535 -0.011870  1.874606 -0.606346   \n",
       "4      0.0            8.0 -0.531738  0.842057 -0.262993  2.330030 -0.583422   \n",
       "\n",
       "        f_4       f_5       f_6       f_7       f_8       f_9      f_10  \\\n",
       "0 -0.203938 -0.413469  0.965623  1.230508  0.114809 -2.012777  0.004936   \n",
       "1 -0.194255  1.771210  1.428127  1.134144  0.114809 -0.219201 -0.351726   \n",
       "2  0.068883 -1.083155  0.979656 -1.125681  0.114809 -1.035376  0.465096   \n",
       "3 -0.586827 -0.815737  0.778096  0.298990  0.114809 -1.176410  0.301204   \n",
       "4 -0.618392 -0.742814 -0.946789  1.230508  0.114809 -0.005858  0.498780   \n",
       "\n",
       "       f_11      f_12      f_13      f_14      f_15      f_16      f_17  \\\n",
       "0  0.284220  0.502155 -0.287932 -1.169338 -0.267310 -0.574423 -0.771869   \n",
       "1  0.846882  0.440299  0.499824  0.893144 -0.010217 -0.681523  1.254092   \n",
       "2  0.150847 -0.044009  0.091257 -1.169338 -0.451820 -0.467322  0.095288   \n",
       "3 -0.015678 -0.656295 -0.150829 -1.169338 -1.121598 -0.681523  2.921973   \n",
       "4 -0.235516 -0.173336  0.106523 -0.138097 -0.829021 -0.826026  4.070158   \n",
       "\n",
       "       f_18      f_19      f_20      f_21      f_22      f_23      f_24  \\\n",
       "0  1.012212 -1.230507  1.785726 -2.090686  0.325659 -0.877769  1.048786   \n",
       "1 -1.026969 -1.690156  0.011152  0.875251  0.325659 -0.458305 -1.797581   \n",
       "2  1.140719 -0.166894 -0.007295 -0.449418  0.325659 -0.682472  0.016262   \n",
       "3  1.179453  0.363938  0.365656 -1.478154  0.325659  0.955586 -0.105029   \n",
       "4  0.148330  0.269455 -0.046685  0.194325  0.325659  1.933630 -1.119676   \n",
       "\n",
       "       f_25      f_26      f_27      f_28  ...     f_268     f_269     f_270  \\\n",
       "0  0.131774 -0.349609 -1.813385  0.099226  ...  0.900593 -0.924766 -1.057890   \n",
       "1 -0.300364  0.584786  0.551460  0.806422  ...  0.171233  1.165891  0.590802   \n",
       "2  0.026124 -0.547330  0.551460 -0.261588  ... -0.161757 -0.114321  0.433277   \n",
       "3 -0.588826  0.650101 -1.813385 -1.421967  ... -1.024947 -0.924766  1.013206   \n",
       "4 -0.585332  0.535749  0.551460 -0.771058  ... -1.508002  1.165891  0.941384   \n",
       "\n",
       "      f_271  f_272     f_273     f_274     f_275     f_276     f_277  \\\n",
       "0 -0.167062    0.0  1.281245  0.258715 -0.237964 -0.742125 -0.324677   \n",
       "1  0.118520    0.0 -0.650803  0.851905  0.086198  1.135668  0.298990   \n",
       "2 -0.207127    0.0  0.098910 -0.725177 -0.388189  0.062644  0.260281   \n",
       "3 -0.324558    0.0 -0.817650 -0.624180  0.012915  0.599156 -1.796107   \n",
       "4 -0.501547    0.0 -1.332959 -0.767390 -0.237964  0.330900 -1.465071   \n",
       "\n",
       "      f_278     f_279     f_280     f_281     f_282     f_283     f_284  \\\n",
       "0  0.992547  0.961355 -0.025610 -0.006259  0.473603  0.040136  0.453711   \n",
       "1 -1.583445 -0.481945  0.532229  0.226693 -0.894744 -0.514552 -1.000073   \n",
       "2  0.980960  0.899393 -0.315315  0.150139  0.245546 -1.429645 -1.000073   \n",
       "3 -1.843759  2.927223 -0.640443  2.324936 -0.666686 -0.049966 -1.000073   \n",
       "4 -0.585437 -1.314835 -0.781270 -1.918721 -0.438628 -1.576060 -1.226833   \n",
       "\n",
       "      f_285     f_286     f_287     f_288     f_289     f_290     f_291  \\\n",
       "0 -1.597790  0.301659  0.157470  0.416631  1.506131  0.366028 -1.095620   \n",
       "1  0.884377 -0.557502 -0.875265 -0.156106  0.537055 -0.154193  0.912726   \n",
       "2 -0.033494 -0.147156 -0.087518  0.098443 -0.529027 -0.138020  0.912726   \n",
       "3 -0.172537 -0.557502 -0.986692 -0.744752  0.237733  0.382201  0.912726   \n",
       "4  1.067312 -0.519032 -1.221407 -0.768617 -0.723919 -0.170365  0.912726   \n",
       "\n",
       "      f_292     f_293     f_294     f_295     f_296     f_297     f_298  \\\n",
       "0  0.200075  0.819155  0.941183 -0.086764 -1.087009 -1.044826 -0.287605   \n",
       "1 -0.734579  0.819155  0.941183 -0.387617 -1.087009 -0.929529 -0.974060   \n",
       "2 -0.551904 -1.220772 -1.060166 -0.219097 -1.087009 -0.612428 -0.113944   \n",
       "3 -0.266359 -1.220772  0.941183 -0.609113  0.104928 -0.783423  1.151730   \n",
       "4 -0.741355 -1.220772  0.941183 -0.588445  0.104928  0.753279  1.345611   \n",
       "\n",
       "      f_299  \n",
       "0  0.321566  \n",
       "1 -0.343624  \n",
       "2  0.243608  \n",
       "3 -0.773309  \n",
       "4 -0.737624  \n",
       "\n",
       "[5 rows x 303 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_features = 300\n",
    "features = [f'f_{i}' for i in range(n_features)]\n",
    "train = pd.read_pickle(r\"C:\\Users\\lab\\Desktop\\ubiquant-market-prediction\\train.pkl\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0   -0.300781\n",
       "1   -0.231079\n",
       "2    0.568848\n",
       "3   -1.064453\n",
       "4   -0.531738\n",
       "Name: target, dtype: float16"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "investment_id = train['investment_id']\n",
    "investment_id.head()\n",
    "\n",
    "y = train['target']\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## custom loss function (ccc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(pred_y, true_y):\n",
    "    pred_mean = tf.math.reduced_mean(pred_y)\n",
    "    true_mean = tf.math.reduced_mean(true_y)\n",
    "\n",
    "    pred_std = tf.math.reduced_std(pred_y)\n",
    "    true_std = tf.math.reduced_std(true_y)\n",
    "\n",
    "    corr_ = tf.math.reduced_sum((pred_y-pred_mean)*(true_y-true_mean))/(len(pred_y)-1)\n",
    "    ccc = (2*corr_*pred_std*true_std)/(pred_std**2+true_std**2+(pred_mean-true_mean)**2)\n",
    "\n",
    "    mse = tf.math.reduce_sum((pred_y-true_y)**2)/tf.cast((len(pred_y)-1),tf.float32)\n",
    "    return 1- ccc + mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a IntegerLookup layer for investment_id input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2789\n"
     ]
    }
   ],
   "source": [
    "investment_ids = list(investment_id.unique())\n",
    "investment_id_size = len(investment_ids) + 1\n",
    "print(investment_id_size)\n",
    "investment_id_lookup_layer = layers.IntegerLookup(max_tokens=investment_id_size)\n",
    "## integerlookup은 \n",
    "investment_id_lookup_layer.adapt(pd.DataFrame({\"investment_ids\":investment_ids})) \n",
    "## 일부 전처리 레이어는 훈련데이터의 셈플을 기반으로 계산해야하는 내부 상태가 있음.\n",
    "## 이러한 전처리 레이어는 '훈련불가능'하기 때문에 훈련중 설정이 되지않음. -> 훈련전에 설정해야함\n",
    "## 이 단계를 적응(adaptation)이라고 함.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def preprocess(X, y):\n",
    "    return X, y\n",
    "def make_dataset(feature, investment_id, y, batch_size=1024, mode=\"train\"):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(((investment_id, feature), y))\n",
    "    ds = ds.map(preprocess)\n",
    "    if mode == \"train\":\n",
    "        ds = ds.shuffle(4096) ## 완벽한 셔플링을 위해서는 셔플링의 크기와 버퍼의 크기가 같아야함\n",
    "    ds = ds.batch(batch_size).cache().prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    investment_id_inputs = tf.keras.Input((1, ), dtype=tf.uint16)\n",
    "    features_inputs = tf.keras.Input((300, ), dtype=tf.float16)\n",
    "    \n",
    "    ## Embedding()을 사용하기 위해서는 입력 될 각 단어들은 모두 정수 인덱싱이 되어 있어야 합니다.\n",
    "    # this code uses variables embedding method because one-hot encoding has many computational costs \n",
    "\n",
    "    investment_id_x = investment_id_lookup_layer(investment_id_inputs)\n",
    "    investment_id_x = layers.Embedding(investment_id_size, 32, input_length=1,embeddings_initializer=\"glorot_uniform\")(investment_id_x)\n",
    "    investment_id_x = layers.Reshape((-1, ))(investment_id_x)\n",
    "\n",
    "    investment_id_x = layers.Dense(64,kernel_initializer = \"glorot_uniform\")(investment_id_x)\n",
    "    investment_id_x = layers.BatchNormalization(64)(investment_id_x)\n",
    "    investment_id_x = layers.Activation('swish')(investment_id_x)\n",
    "\n",
    "    investment_id_x = layers.Dense(64,kernel_initializer = \"glorot_uniform\")(investment_id_x)\n",
    "    investment_id_x = layers.BatchNormalization(64)(investment_id_x)\n",
    "    investment_id_x = layers.Activation('swish')(investment_id_x)\n",
    "\n",
    "    investment_id_x = layers.Dense(64,kernel_initializer = \"glorot_uniform\")(investment_id_x)\n",
    "    investment_id_x = layers.BatchNormalization(64)(investment_id_x)\n",
    "    investment_id_x = layers.Activation('swish')(investment_id_x)\n",
    "\n",
    "\n",
    "    feature_x = layers.Dense(256,kernel_initializer = \"glorot_uniform\")(features_inputs)\n",
    "    feature_x = layers.BatchNormalization(256)(feature_x)\n",
    "    feature_x = layers.Activation('swish')(feature_x)\n",
    "\n",
    "    feature_x = layers.Dense(256,kernel_initializer = \"glorot_uniform\")(feature_x)\n",
    "    feature_x = layers.BatchNormalization(256)(feature_x)\n",
    "    feature_x = layers.Activation('swish')(feature_x)\n",
    "\n",
    "    feature_x = layers.Dense(256,kernel_initializer = \"glorot_uniform\")(feature_x)\n",
    "    feature_x = layers.BatchNormalization(256)(feature_x)\n",
    "    feature_x = layers.Activation('swish')(feature_x)\n",
    "\n",
    "\n",
    "    \n",
    "    x = layers.Concatenate(axis=1)([investment_id_x, feature_x])\n",
    "\n",
    "    x = layers.Dense(512,kernel_initializer = \"glorot_uniform\", kernel_regularizer=\"l2\")(x)\n",
    "    x = layers.BatchNormalization(512)(x)\n",
    "    x = layers.Activation('swish')(x)\n",
    "\n",
    "    x = layers.Dense(128,kernel_initializer = \"glorot_uniform\", kernel_regularizer=\"l2\")(x)\n",
    "    x = layers.BatchNormalization(128)(x)\n",
    "    x = layers.Activation('swish')(x)\n",
    "\n",
    "    x = layers.Dense(32,kernel_initializer = \"glorot_uniform\", kernel_regularizer=\"l2\")(x)\n",
    "    x = layers.BatchNormalization(32)(x)\n",
    "    x = layers.Activation('swish')(x)\n",
    "    \n",
    "    output = layers.Dense(1)(x)\n",
    "    rmse = keras.metrics.RootMeanSquaredError(name=\"rmse\")\n",
    "    model = tf.keras.Model(inputs=[investment_id_inputs, features_inputs], outputs=[output])\n",
    "    model.compile(optimizer=tf.optimizers.Adam(0.001), loss=custom_loss, metrics=['mse', \"mae\", \"mape\", rmse, custom_loss])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_17 (InputLayer)          [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " integer_lookup (IntegerLookup)  (None, 1)           0           ['input_17[0][0]']               \n",
      "                                                                                                  \n",
      " embedding_7 (Embedding)        (None, 1, 32)        89248       ['integer_lookup[7][0]']         \n",
      "                                                                                                  \n",
      " reshape_7 (Reshape)            (None, 32)           0           ['embedding_7[0][0]']            \n",
      "                                                                                                  \n",
      " input_18 (InputLayer)          [(None, 300)]        0           []                               \n",
      "                                                                                                  \n",
      " dense_70 (Dense)               (None, 64)           2112        ['reshape_7[0][0]']              \n",
      "                                                                                                  \n",
      " dense_73 (Dense)               (None, 256)          77056       ['input_18[0][0]']               \n",
      "                                                                                                  \n",
      " dense_71 (Dense)               (None, 64)           4160        ['dense_70[0][0]']               \n",
      "                                                                                                  \n",
      " dense_74 (Dense)               (None, 256)          65792       ['dense_73[0][0]']               \n",
      "                                                                                                  \n",
      " dense_72 (Dense)               (None, 64)           4160        ['dense_71[0][0]']               \n",
      "                                                                                                  \n",
      " dense_75 (Dense)               (None, 256)          65792       ['dense_74[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_7 (Concatenate)    (None, 320)          0           ['dense_72[0][0]',               \n",
      "                                                                  'dense_75[0][0]']               \n",
      "                                                                                                  \n",
      " dense_76 (Dense)               (None, 512)          164352      ['concatenate_7[0][0]']          \n",
      "                                                                                                  \n",
      " dense_77 (Dense)               (None, 128)          65664       ['dense_76[0][0]']               \n",
      "                                                                                                  \n",
      " dense_78 (Dense)               (None, 32)           4128        ['dense_77[0][0]']               \n",
      "                                                                                                  \n",
      " dense_79 (Dense)               (None, 1)            33          ['dense_78[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 542,497\n",
      "Trainable params: 542,497\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.summary()\n",
    "keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Anaconda3\\envs\\tw_tf\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train : \t               f_0       f_1       f_2       f_3       f_4       f_5       f_6  \\\n",
      "0        0.932573  0.113691 -0.402206  0.378386 -0.203938 -0.413469  0.965623   \n",
      "1        0.810802 -0.514115  0.742368 -0.616673 -0.194255  1.771210  1.428127   \n",
      "2        0.393974  0.615937  0.567806 -0.607963  0.068883 -1.083155  0.979656   \n",
      "3       -2.343535 -0.011870  1.874606 -0.606346 -0.586827 -0.815737  0.778096   \n",
      "6       -1.863797  0.113691  1.573864 -0.598433 -0.569936  0.398784  0.054528   \n",
      "...           ...       ...       ...       ...       ...       ...       ...   \n",
      "3141404  0.892171 -1.760851  0.135189 -0.405799 -0.214687  0.142001  1.134768   \n",
      "3141405  0.093530 -0.720275 -0.345497 -0.438781 -0.166972 -0.437182  1.475746   \n",
      "3141406 -1.344935 -0.199987 -0.107702 -0.454677 -0.221914 -0.141174 -1.498235   \n",
      "3141408 -2.565332  0.320301  0.076600  1.380182 -0.155366 -0.689000  0.381069   \n",
      "3141409 -0.089557  0.190229 -0.548256  0.151205  0.079773  0.447962  1.014983   \n",
      "\n",
      "              f_7       f_8       f_9  ...     f_290     f_291     f_292  \\\n",
      "0        1.230508  0.114809 -2.012777  ...  0.366028 -1.095620  0.200075   \n",
      "1        1.134144  0.114809 -0.219201  ... -0.154193  0.912726 -0.734579   \n",
      "2       -1.125681  0.114809 -1.035376  ... -0.138020  0.912726 -0.551904   \n",
      "3        0.298990  0.114809 -1.176410  ...  0.382201  0.912726 -0.266359   \n",
      "6        0.724549  0.114809  0.318106  ...  0.821560  0.912726  0.476309   \n",
      "...           ...       ...       ...  ...       ...       ...       ...   \n",
      "3141404 -0.517340  0.056425  1.097691  ...  0.639348 -1.232434  0.311625   \n",
      "3141405  1.284423  0.056425 -1.433681  ... -0.285908 -1.232434 -0.660579   \n",
      "3141406  1.373834  0.056425 -1.211572  ...  0.184517 -1.232434 -0.670493   \n",
      "3141408 -1.324759  0.056425 -1.111730  ... -0.756332 -1.232434  0.133074   \n",
      "3141409 -1.324759  0.056425 -1.952123  ... -0.317095  0.811402  3.271590   \n",
      "\n",
      "            f_293     f_294     f_295     f_296     f_297     f_298     f_299  \n",
      "0        0.819155  0.941183 -0.086764 -1.087009 -1.044826 -0.287605  0.321566  \n",
      "1        0.819155  0.941183 -0.387617 -1.087009 -0.929529 -0.974060 -0.343624  \n",
      "2       -1.220772 -1.060166 -0.219097 -1.087009 -0.612428 -0.113944  0.243608  \n",
      "3       -1.220772  0.941183 -0.609113  0.104928 -0.783423  1.151730 -0.773309  \n",
      "6       -1.220772  0.941183 -0.434315  1.296864  0.171329  1.051288 -0.745335  \n",
      "...           ...       ...       ...       ...       ...       ...       ...  \n",
      "3141404  0.875537  0.421628 -0.332911  1.363181 -0.075892 -1.420459 -0.521622  \n",
      "3141405  0.875537  0.421628 -0.428097 -0.075548 -0.533092 -0.193732 -0.581394  \n",
      "3141406  0.875537  0.421628 -0.729949 -1.514277  0.013145 -0.890270 -0.589705  \n",
      "3141408 -1.142157  0.421628 -0.375288 -1.514277 -0.973762  0.608647 -0.372040  \n",
      "3141409  0.875537  0.421628 -0.170709  1.363181 -0.563314  0.669586  0.456400  \n",
      "\n",
      "[2513128 rows x 300 columns]\n",
      "X_val : \t               f_0       f_1       f_2       f_3       f_4       f_5       f_6  \\\n",
      "4        0.842057 -0.262993  2.330030 -0.583422 -0.618392 -0.742814 -0.946789   \n",
      "5        0.608855  1.369305 -0.761515  0.865860 -0.359269 -1.835762  1.384409   \n",
      "10       0.686414 -0.514115 -1.465751  0.505034  3.336757 -1.988232 -0.444304   \n",
      "11       0.081737  1.745989 -1.218067  1.380061 -0.036835 -1.552630  0.991577   \n",
      "15       0.000000  3.503847  0.776656 -0.576955  0.398469  2.639607 -0.920601   \n",
      "...           ...       ...       ...       ...       ...       ...       ...   \n",
      "3141388  0.447173 -0.460131  0.197822 -0.432405 -0.029318 -1.600465 -0.566944   \n",
      "3141393 -1.300545  1.490949 -0.621157  0.276278 -0.140592  0.927713 -1.415919   \n",
      "3141394 -0.089364 -0.069915  1.411002 -0.405366  0.152336 -0.186410 -1.812565   \n",
      "3141399  0.829150 -0.980419  0.137483 -0.448415 -0.222940 -0.177434  0.241221   \n",
      "3141407  0.979489 -1.110491  1.006980 -0.467307 -0.159549  1.355671  0.150812   \n",
      "\n",
      "              f_7       f_8       f_9  ...     f_290     f_291     f_292  \\\n",
      "4        1.230508  0.114809 -0.005858  ... -0.170365  0.912726 -0.741355   \n",
      "5       -1.789227  0.114809 -1.900184  ...  0.333684 -1.095620 -0.335999   \n",
      "10      -1.789227  0.114809  0.801366  ... -0.658241  0.912726 -1.137827   \n",
      "11      -0.710902  0.114809 -0.501794  ...  0.349856 -1.095620  0.000378   \n",
      "15       0.602612  0.114809  0.344168  ... -0.105676 -1.095620  2.194626   \n",
      "...           ...       ...       ...  ...       ...       ...       ...   \n",
      "3141388 -0.181319  0.056425 -1.187807  ... -0.756332  0.811402 -0.747365   \n",
      "3141393 -1.926414  0.056425 -1.058461  ... -0.756332  0.811402  0.004035   \n",
      "3141394  0.476270  0.056425  0.240736  ...  0.639348  0.811402 -0.350773   \n",
      "3141399  1.427723  0.056425 -1.743312  ...  0.184517 -1.232434  0.119316   \n",
      "3141407 -0.088923  0.056425  0.996380  ... -0.756332 -1.232434  0.820784   \n",
      "\n",
      "            f_293     f_294     f_295     f_296     f_297     f_298     f_299  \n",
      "4       -1.220772  0.941183 -0.588445  0.104928  0.753279  1.345611 -0.737624  \n",
      "5        0.819155 -1.060166 -0.343812 -1.087009  0.077862  0.142943 -0.055550  \n",
      "10       0.819155  0.941183  1.132590  0.104928  1.174890  1.085646  2.405461  \n",
      "11       0.819155 -1.060166  0.210915 -1.087009 -0.367434  0.537452  0.243616  \n",
      "15       0.819155 -1.477153  0.653271  1.296864 -0.247746  0.000000  1.422416  \n",
      "...           ...       ...       ...       ...       ...       ...       ...  \n",
      "3141388 -1.142157  0.421628 -0.272396  1.363181  0.014085  0.710105 -0.229014  \n",
      "3141393  0.875537  0.421628 -0.304018  1.363181 -0.852834  0.119049 -0.259806  \n",
      "3141394  0.875537  0.421628  0.575612  1.363181 -0.089519  1.105934  0.181167  \n",
      "3141399 -1.142157  0.421628 -0.704748 -0.075548 -0.091505 -1.000041 -0.641396  \n",
      "3141407 -1.142157  0.421628 -0.363329  1.363181 -0.079106 -1.580124 -0.297625  \n",
      "\n",
      "[628282 rows x 300 columns]\n",
      "investment_id : \t 0             1.0\n",
      "1             2.0\n",
      "2             6.0\n",
      "3             7.0\n",
      "4             8.0\n",
      "            ...  \n",
      "3141405    3768.0\n",
      "3141406    3768.0\n",
      "3141407    3770.0\n",
      "3141408    3772.0\n",
      "3141409    3772.0\n",
      "Name: investment_id, Length: 3141410, dtype: float16\n",
      "investment_id_train : \t 0             1.0\n",
      "1             2.0\n",
      "2             6.0\n",
      "3             7.0\n",
      "6            10.0\n",
      "            ...  \n",
      "3141404    3766.0\n",
      "3141405    3768.0\n",
      "3141406    3768.0\n",
      "3141408    3772.0\n",
      "3141409    3772.0\n",
      "Name: investment_id, Length: 2513128, dtype: float16\n",
      "y_train : \t 0         -0.300781\n",
      "1         -0.231079\n",
      "2          0.568848\n",
      "3         -1.064453\n",
      "6         -0.260742\n",
      "             ...   \n",
      "3141404    0.351807\n",
      "3141405    0.033600\n",
      "3141406   -0.223267\n",
      "3141408    0.009598\n",
      "3141409    1.211914\n",
      "Name: target, Length: 2513128, dtype: float16\n",
      "y_val : \t 4         -0.531738\n",
      "5          1.505859\n",
      "10        -0.130493\n",
      "11         0.365479\n",
      "15         0.171631\n",
      "             ...   \n",
      "3141388   -0.117432\n",
      "3141393    0.164307\n",
      "3141394    0.127563\n",
      "3141399   -0.167969\n",
      "3141407   -0.559570\n",
      "Name: target, Length: 628282, dtype: float16\n",
      "investment_id_val : \t 4             8.0\n",
      "5             9.0\n",
      "10           16.0\n",
      "11           17.0\n",
      "15           24.0\n",
      "            ...  \n",
      "3141388    3748.0\n",
      "3141393    3754.0\n",
      "3141394    3756.0\n",
      "3141399    3760.0\n",
      "3141407    3770.0\n",
      "Name: investment_id, Length: 628282, dtype: float16\n",
      "Epoch 1/30\n",
      " 651/2455 [======>.......................] - ETA: 55s - loss: 1.1253 - mse: 0.8387 - mae: 0.6455 - mape: 89110.4141 - rmse: 0.9158"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "kfold = StratifiedKFold(5, shuffle=True, random_state=42)\n",
    "models_dnn = []\n",
    "for index, (train_indices, valid_indices) in enumerate(kfold.split(train, investment_id)):\n",
    "    #if args.INFER == False:\n",
    "    X_train, X_val = train[features].iloc[train_indices], train[features].iloc[valid_indices]\n",
    "    investment_id_train = investment_id[train_indices]\n",
    "    y_train, y_val = y.iloc[train_indices], y.iloc[valid_indices]\n",
    "    investment_id_val = investment_id[valid_indices]\n",
    "    if args.INFER == False:\n",
    "        train_ds = make_dataset(X_train, investment_id_train, y_train)\n",
    "    valid_ds = make_dataset(X_val, investment_id_val, y_val, mode=\"valid\")\n",
    "    model = get_model()\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(f\"{args.dnn_path}/model_{index}.tf\", save_best_only=True)\n",
    "    early_stop = keras.callbacks.EarlyStopping(patience=10)\n",
    "    if args.INFER == False:\n",
    "        history = model.fit(train_ds, epochs=30, validation_data=valid_ds, callbacks=[checkpoint, early_stop])\n",
    "    model.load_weights(f\"{args.dnn_path}/model_{index}.tf\")\n",
    "    models_dnn.append(model)\n",
    "    \n",
    "    pearson_score = stats.pearsonr(model.predict(valid_ds).ravel(), y_val.values)[0]\n",
    "    print('Pearson:', pearson_score)\n",
    "    if args.INFER == False:\n",
    "        pd.DataFrame(history.history, columns=[\"mse\", \"val_mse\"]).plot()\n",
    "        plt.title(\"MSE\")\n",
    "        plt.show()\n",
    "        pd.DataFrame(history.history, columns=[\"mae\", \"val_mae\"]).plot()\n",
    "        plt.title(\"MAE\")\n",
    "        plt.show()\n",
    "        pd.DataFrame(history.history, columns=[\"rmse\", \"val_rmse\"]).plot()\n",
    "        plt.title(\"RMSE\")\n",
    "        plt.show()\n",
    "        \n",
    "    del investment_id_train\n",
    "    del investment_id_val\n",
    "    del X_train\n",
    "    del X_val\n",
    "    del y_train\n",
    "    del y_val\n",
    "    if args.INFER == False:\n",
    "        del train_ds\n",
    "    del valid_ds\n",
    "    gc.collect()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Time ID DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>investment_id</th>\n",
       "      <th colspan=\"2\" halign=\"left\">time_id</th>\n",
       "      <th>time_span</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>757.0</td>\n",
       "      <td>1219.0</td>\n",
       "      <td>462.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1219.0</td>\n",
       "      <td>1219.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1219.0</td>\n",
       "      <td>1219.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1219.0</td>\n",
       "      <td>1195.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>885.0</td>\n",
       "      <td>1219.0</td>\n",
       "      <td>334.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1219.0</td>\n",
       "      <td>1219.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  investment_id time_id         time_span\n",
       "                    min     max          \n",
       "0           0.0   757.0  1219.0     462.0\n",
       "1           1.0     0.0  1219.0    1219.0\n",
       "2           2.0     0.0  1219.0    1219.0\n",
       "3           3.0    24.0  1219.0    1195.0\n",
       "4           4.0   885.0  1219.0     334.0\n",
       "5           6.0     0.0  1219.0    1219.0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_id_df = (\n",
    "    train.filter(regex=r\"^(?!f_).*\")\n",
    "    .groupby(\"investment_id\")\n",
    "    .agg({\"time_id\": [\"min\", \"max\"]})\n",
    "    .reset_index()\n",
    ")\n",
    "time_id_df[\"time_span\"] = time_id_df[\"time_id\"].diff(axis=1)[\"max\"]\n",
    "time_id_df.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     investment_id time_span\n",
      "                            \n",
      "0              0.0     462.0\n",
      "1              1.0    1219.0\n",
      "2              2.0    1219.0\n",
      "3              3.0    1195.0\n",
      "4              4.0     334.0\n",
      "...            ...       ...\n",
      "2783        3764.0    1219.0\n",
      "2784        3766.0    1219.0\n",
      "2785        3768.0    1219.0\n",
      "2786        3770.0    1219.0\n",
      "2787        3772.0    1219.0\n",
      "\n",
      "[2788 rows x 2 columns]\n",
      "      investment_id  time_span\n",
      "0               0.0      462.0\n",
      "1               1.0     1219.0\n",
      "2               2.0     1219.0\n",
      "3               3.0     1195.0\n",
      "4               4.0      334.0\n",
      "...             ...        ...\n",
      "2783         3764.0     1219.0\n",
      "2784         3766.0     1219.0\n",
      "2785         3768.0     1219.0\n",
      "2786         3770.0     1219.0\n",
      "2787         3772.0     1219.0\n",
      "\n",
      "[2788 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(time_id_df.drop(columns=\"time_id\"))\n",
    "print(time_id_df.drop(columns=\"time_id\").droplevel(level=1, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Time ID DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1804"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1152x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train = train.merge(time_id_df.drop(columns=\"time_id\").droplevel(level=1, axis=1), on=\"investment_id\")\n",
    "## drop level은 time id 지우고 나서 한 칸 남아서 남은칸 삭제\n",
    "train.time_span.hist(bins=args.num_bins, figsize=(16,8))\n",
    "del time_id_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"fold\"] = -1\n",
    "_target = pd.cut(train.time_span, args.num_bins, labels=False)\n",
    "skf = StratifiedKFold(n_splits=args.folds)\n",
    "for fold, (train_index, valid_index) in enumerate(skf.split(_target, _target)):\n",
    "    train.loc[valid_index, 'fold'] = fold\n",
    "    \n",
    "fig, axs = plt.subplots(nrows=args.folds, ncols=1, sharex=True, figsize=(16,8), tight_layout=True)\n",
    "for ax, (fold, df) in zip(axs, train[[\"fold\", \"time_span\"]].groupby(\"fold\")):\n",
    "    ax.hist(df.time_span, bins=args.num_bins)\n",
    "    ax.text(0, 40000, f\"fold: {fold}, count: {len(df)}\", fontsize=16)\n",
    "plt.show()\n",
    "del _target, train_index, valid_index\n",
    "_=gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop(['target', 'time_id'], axis = 1)\n",
    "y = train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = [f\"f_{i}\" for i in range(300)]\n",
    "cat_features = [\"investment_id\"]\n",
    "features = num_features + cat_features\n",
    "features += [\"time_id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TabNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder, StandardScaler\n",
    "import torch\n",
    "from torch.optim import Adam, SGD\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingWarmRestarts\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return mean_squared_error(y_true,y_pred, squared=False)\n",
    "def rmspe(y_true, y_pred):\n",
    "    # Function to calculate the root mean squared percentage error\n",
    "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
    "\n",
    "class RMSPE(Metric):\n",
    "    def __init__(self):\n",
    "        self._name = \"rmspe\"\n",
    "        self._maximize = False\n",
    "\n",
    "    def __call__(self, y_true, y_score):\n",
    "        \n",
    "        return np.sqrt(np.mean(np.square((y_true - y_score) / y_true)))\n",
    "    \n",
    "\n",
    "def RMSPELoss(y_pred, y_true):\n",
    "    return torch.sqrt(torch.mean( ((y_true - y_pred) / y_true) ** 2 )).clone()\n",
    "\n",
    "\n",
    "\n",
    "cat_idxs = [ i for i, f in enumerate(X.columns.tolist()) if f in cat_features]\n",
    "\n",
    "\n",
    "def run():    \n",
    "    tabnet_params = dict(\n",
    "        cat_idxs=cat_idxs,\n",
    "        cat_emb_dim=1,\n",
    "        n_d = 16,\n",
    "        n_a = 16,\n",
    "        n_steps = 2,\n",
    "        gamma =1.4690246460970766,\n",
    "        n_independent = 9,\n",
    "        n_shared = 4,\n",
    "        lambda_sparse = 0,\n",
    "        optimizer_fn = Adam,\n",
    "        optimizer_params = dict(lr = (0.024907164557092944)),\n",
    "        mask_type = \"entmax\",\n",
    "        scheduler_params = dict(T_0=200, T_mult=1, eta_min=1e-4, last_epoch=-1, verbose=False),\n",
    "        scheduler_fn = CosineAnnealingWarmRestarts,\n",
    "        seed = 42,\n",
    "        verbose = 10, \n",
    "    )    \n",
    "    y = train['target']\n",
    "    train['preds'] = -1000\n",
    "    scores = defaultdict(list)\n",
    "    features_importance= pd.DataFrame()\n",
    "    \n",
    "    for fold in range(args.folds):\n",
    "        print(f\"=====================fold: {fold}=====================\")\n",
    "        trn_ind, val_ind = train.fold!=fold, train.fold==fold\n",
    "        print(f\"train length: {trn_ind.sum()}, valid length: {val_ind.sum()}\")\n",
    "        X_train=train.loc[trn_ind, features].values\n",
    "        y_train=y.loc[trn_ind].values.reshape(-1,1)\n",
    "        X_val=train.loc[val_ind, features].values\n",
    "        y_val=y.loc[val_ind].values.reshape(-1,1)\n",
    "\n",
    "        clf =  TabNetRegressor(**tabnet_params)\n",
    "        clf.fit(\n",
    "          X_train, y_train,\n",
    "          eval_set=[(X_val, y_val)],\n",
    "          max_epochs = 355,\n",
    "          patience = 50,\n",
    "          batch_size = 1024*20, \n",
    "          virtual_batch_size = 128*20,\n",
    "          num_workers = 4,\n",
    "          drop_last = False,\n",
    "\n",
    "          )\n",
    "        \n",
    "        clf.save_model(f'TabNet_seed{args.seed}_{fold}')\n",
    "\n",
    "\n",
    "        preds = clf.predict(train.loc[val_ind, features].values)\n",
    "        train.loc[val_ind, \"preds\"] = preds\n",
    "        \n",
    "        scores[\"rmse\"].append(rmse(y.loc[val_ind], preds))\n",
    "     \n",
    "        del X_train,X_val,y_train,y_val\n",
    "        gc.collect()\n",
    "        \n",
    "        \n",
    "    print(f\"TabNet {args.folds} folds mean rmse: {np.mean(scores['rmse'])}\")\n",
    "    train.filter(regex=r\"^(?!f_).*\").to_csv(\"preds.csv\", index=False)\n",
    " #   return features_importance\n",
    "if args.INFER:\n",
    "    pass\n",
    "else:\n",
    "    run()  \n",
    "#del df, train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    " \n",
    "def zipDir(dirpath, outFullName):\n",
    "\n",
    "    zip = zipfile.ZipFile(outFullName, \"w\", zipfile.ZIP_DEFLATED)\n",
    "    for path, dirnames, filenames in os.walk(dirpath):\n",
    "\n",
    "        fpath = path.replace(dirpath, '')\n",
    "\n",
    "        for filename in filenames:\n",
    "            zip.write(os.path.join(path, filename), os.path.join(fpath, filename))\n",
    "    zip.close()\n",
    "    \n",
    "\n",
    "if args.INFER:\n",
    "    for fold in range(5):\n",
    "        input_path =f'{args.tabnet_path}/fold{fold}'\n",
    "        output_path = f\"./fold{fold}.zip\"\n",
    "        zipDir(input_path, output_path)\n",
    "else:\n",
    "    input_path =f'./TabNet_seed{args.seed}_{fold}'\n",
    "    output_path = f\"./fold{fold}.zip\"\n",
    "\n",
    "    zipDir(input_path, output_path)\n",
    "tabnet_params = dict(\n",
    "        cat_idxs=cat_idxs,\n",
    "        cat_emb_dim=1,\n",
    "        n_d = 16,\n",
    "        n_a = 16,\n",
    "        n_steps = 2,\n",
    "        gamma =1.4690246460970766,\n",
    "        n_independent = 9,\n",
    "        n_shared = 4,\n",
    "        lambda_sparse = 0,\n",
    "        optimizer_fn = Adam,\n",
    "        optimizer_params = dict(lr = (0.024907164557092944)),\n",
    "        mask_type = \"entmax\",\n",
    "        scheduler_params = dict(T_0=200, T_mult=1, eta_min=1e-4, last_epoch=-1, verbose=False),\n",
    "        scheduler_fn = CosineAnnealingWarmRestarts,\n",
    "        seed = 42,\n",
    "        verbose = 10, \n",
    "    )    \n",
    "\n",
    "\n",
    "\n",
    "import copy\n",
    "clf =  TabNetRegressor(**tabnet_params)\n",
    "models_tabnet = []\n",
    "for fold in range(args.folds):\n",
    "    clf.load_model(f\"fold{fold}.zip\")\n",
    "    model=copy.deepcopy(clf)\n",
    "    models_tabnet.append(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare Test Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_test(investment_id, feature):\n",
    "    return (investment_id, feature), 0\n",
    "\n",
    "def make_test_dataset(feature, investment_id, batch_size=1024):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(((investment_id, feature)))\n",
    "    ds = ds.map(preprocess_test)\n",
    "    ds = ds.batch(batch_size).cache().prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "def make_test_dataset_lgbm(test_df,folds=5):\n",
    "    features = [f\"f_{i}\" for i in range(300)]\n",
    "    test_df[features] = scaler.fit_transform(test_df[features]) \n",
    "    clu = [kmodels[fold].predict(test_df[features]) for fold in range(folds)]\n",
    "    test_df_l = [test_df for fold in range(folds)]\n",
    "    for f in range(folds):\n",
    "        test_df_l[f]['cluster'] = clu[f]\n",
    "    return test_df_l\n",
    "\n",
    "def inference(models, ds):\n",
    "    y_preds = []\n",
    "    for model in models:\n",
    "        y_pred = model.predict(ds)\n",
    "        y_preds.append(y_pred)\n",
    "    return np.mean(y_preds, axis=0)\n",
    "\n",
    "def inference_lgbm(models,ds,folds=5):\n",
    "    features = [f\"f_{i}\" for i in range(300)]\n",
    "    features_1 = features + ['cluster']\n",
    "    final_pred = [models[fold].predict(ds[fold][features_1]) for fold in range(folds)]\n",
    "    return np.mean(np.stack(final_pred), axis=0)\n",
    "\n",
    "def inference_tabnet(models,test_df,args):\n",
    "    num_features = [f\"f_{i}\" for i in range(300)]\n",
    "    cat_features = [\"investment_id\"]\n",
    "    features = num_features + cat_features\n",
    "    features += [\"time_id\"]\n",
    "    test_df[\"time_id\"] = test_df.row_id.str.extract(r\"(\\d+)_.*\").astype(np.uint16) # extract time_id form row_id\n",
    "    final_pred = [models[fold].predict(test_df[features].values) for fold in range(args.folds)]\n",
    "    return np.mean(np.stack(final_pred), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.7.11 64-bit ('py37_taewon_tf': conda)' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: 'conda install -n py37_taewon_tf ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "def preprocess_test(investment_id, feature):\n",
    "    return (investment_id, feature), 0\n",
    "def make_test_dataset(feature, investment_id, batch_size=1024):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(((investment_id, feature)))\n",
    "    ds = ds.map(preprocess_test)\n",
    "    ds = ds.batch(batch_size).cache().prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    return ds\n",
    "def inference(models, ds):\n",
    "    y_preds = []\n",
    "    for model in models:\n",
    "        y_pred = model.predict(ds)\n",
    "        y_preds.append(y_pred)\n",
    "    return np.mean(y_preds, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ubiquant\n",
    "env = ubiquant.make_env()\n",
    "iter_test = env.iter_test() \n",
    "for (test_df, sample_prediction_df) in iter_test:\n",
    "\n",
    "    features_dnn = [f'f_{i}' for i in range(300)]\n",
    "    ds = make_test_dataset(test_df[features_dnn], test_df[\"investment_id\"])\n",
    "\n",
    "    tabnet_output = inference_tabnet(models_tabnet,test_df, args)\n",
    "    dnn_output = inference(models_dnn, ds)\n",
    "    final_output = dnn_output * 0.4 + tabnet_output *0.6\n",
    "    #final_output = tabnet_output\n",
    "    sample_prediction_df['target'] = final_output\n",
    "    env.predict(sample_prediction_df) "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "204802287329ac0cf798540e345e8505329bf490b7b9371d7044f6322e5f6def"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('tw_tf': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
